# -*- coding: utf-8 -*-
"""betty.james (23 Apr 2025, 11:14:12)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/data-science-911/locations/us-central1/repositories/30ddb6a0-ad02-44e3-88a9-b635b57a26a9

**LSTM**
"""

!pip install poliastro
!pip install astropy

import pandas as pd
import numpy as np
from datetime import datetime
from scipy.interpolate import interp1d
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
from astropy import units as u
from astropy.coordinates import CartesianRepresentation
from poliastro.twobody import Orbit
from poliastro.bodies import Earth

# =============================================
# 1. Custom DateTime Parser for Your Format
# =============================================
def parse_custom_datetime(dt_str):
    """Parse dates in format: '2022 244 (01 SEP) 01:38:58.810'"""
    try:
        # Extract the YYYY DOY part
        year, doy = map(int, dt_str.split()[:2])
        # Convert DOY to month/day
        base_date = datetime(year, 1, 1)
        parsed_date = base_date + pd.Timedelta(days=doy-1)
        # Add the time component
        time_part = dt_str.split(') ')[1]
        time_obj = datetime.strptime(time_part, '%H:%M:%S.%f').time()
        return datetime.combine(parsed_date.date(), time_obj)
    except Exception as e:
        print(f"Error parsing {dt_str}: {e}")
        return pd.NaT

# =============================================
# 2. Data Loading and Preparation
# =============================================
def load_and_prepare_data(csv_path):
    # Load data with custom datetime parsing
    df = pd.read_csv(csv_path, converters={'EPOCH TIME (UTC)': parse_custom_datetime})

    # Drop rows with invalid dates
    df = df[df['EPOCH TIME (UTC)'].notna()]

    # Time handling
    df['time'] = (df['EPOCH TIME (UTC)'] - df['EPOCH TIME (UTC)'].min()).dt.total_seconds()

    # Column renaming
    column_mapping = {
        'SATELLITE NUMBER': 'obj_id',
        'J2K POS X (KM)': 'pos_x',
        'J2K POS Y (KM)': 'pos_y',
        'J2K POS Z (KM)': 'pos_z',
        'J2K VEL X (KM/S)': 'vel_x',
        'J2K VEL Y (KM/S)': 'vel_y',
        'J2K VEL Z (KM/S)': 'vel_z',
        'BALLISTIC COEF (M2/KG)': 'BC',
        'SOLAR RAD PRESS COEFF (M2/KG)': 'C_SRP',
        'OBJECT_TYPE': 'class'
    }
    df = df.rename(columns=column_mapping)

    # Calculate orbital periods
    print("Calculating orbital periods...")
    df['TP'] = np.nan
    for obj_id in df['obj_id'].unique():
        obj_data = df[df['obj_id'] == obj_id].sort_values('time')
        if len(obj_data) > 1:
            try:
                pos = CartesianRepresentation(
                    obj_data['pos_x'].values * u.km,
                    obj_data['pos_y'].values * u.km,
                    obj_data['pos_z'].values * u.km
                )
                vel = CartesianRepresentation(
                    obj_data['vel_x'].values * (u.km/u.s),
                    obj_data['vel_y'].values * (u.km/u.s),
                    obj_data['vel_z'].values * (u.km/u.s)
                )
                orb = Orbit.from_vectors(Earth, pos[0], vel[0])
                df.loc[df['obj_id'] == obj_id, 'TP'] = orb.period.to(u.min).value
            except Exception as e:
                # Fallback approximation
                avg_altitude = np.mean(np.linalg.norm(
                    obj_data[['pos_x','pos_y','pos_z']], axis=1)) - 6371
                avg_velocity = np.mean(np.linalg.norm(
                    obj_data[['vel_x','vel_y','vel_z']], axis=1))
                df.loc[df['obj_id'] == obj_id, 'TP'] = 2 * np.pi * (avg_altitude + 6371) / avg_velocity / 60

    return df

# =============================================
# 3. Time-Series Generation
# =============================================
def generate_time_series(df, n_steps=12, step_size=300):
    features = ['pos_x','pos_y','pos_z','vel_x','vel_y','vel_z','BC','C_SRP']
    sequences = []
    labels = []

    for obj_id in df['obj_id'].unique():
        obj_data = df[df['obj_id'] == obj_id].sort_values('time')
        if len(obj_data) < 2:
            continue

        # Create time points for propagation
        min_time = obj_data['time'].min()
        max_time = obj_data['time'].max()
        tp = obj_data['TP'].iloc[0]

        # Generate 4 snapshots per orbit
        for base_time in np.arange(min_time, max_time, tp/4):
            times = base_time + np.arange(-n_steps//2, n_steps//2) * step_size

            # Interpolate features
            seq_data = []
            for t in times:
                if t < min_time or t > max_time:
                    continue

                point = {}
                for feat in features:
                    f = interp1d(obj_data['time'], obj_data[feat],
                                kind='linear', fill_value='extrapolate')
                    point[feat] = float(f(t))

                seq_data.append(point)

            if len(seq_data) == n_steps:
                sequences.append(np.array([[p[feat] for feat in features] for p in seq_data]))
                labels.append(obj_data['class'].iloc[0])

    return np.stack(sequences), np.array(labels)

# =============================================
# 4. Model Building
# =============================================
def build_lstm_model(input_shape, n_classes):
    model = Sequential([
        Masking(mask_value=-999, input_shape=input_shape),
        LSTM(64, return_sequences=True, dropout=0.2),
        LSTM(32, dropout=0.2),
        Dense(n_classes, activation='softmax')
    ])
    model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
    return model

def create_voting_classifier(input_shape, n_classes, class_weights):
    # LSTM
    lstm_model = KerasClassifier(
        build_fn=lambda: build_lstm_model(input_shape, n_classes),
        epochs=30,
        batch_size=64,
        verbose=0,
        callbacks=[EarlyStopping(patience=3)]
    )

    # Random Forest
    rf_model = RandomForestClassifier(
        n_estimators=200,
        class_weight=class_weights,
        max_depth=10,
        n_jobs=-1
    )

    # XGBoost
    xgb_model = XGBClassifier(
        n_estimators=150,
        scale_pos_weight=class_weights[1]/class_weights[0],
        use_label_encoder=False,
        eval_metric='mlogloss'
    )

    return VotingClassifier(
        estimators=[
            ('lstm', lstm_model),
            ('rf', rf_model),
            ('xgb', xgb_model)
        ],
        voting='soft'
    )

# =============================================
# 5. Main Execution
# =============================================
# Load and prepare data
print("Loading data...")
df = load_and_prepare_data('gs://betty_101/processed_space_objects_data.csv')

# Generate time-series data
print("Generating time-series...")
X, y = generate_time_series(df)

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Calculate class weights
classes = np.unique(y_encoded)
class_weights = compute_class_weight('balanced', classes=classes, y=y_encoded)
class_weight_dict = dict(zip(classes, class_weights))

# Normalize features
scaler = StandardScaler()
X_reshaped = X.reshape(-1, X.shape[-1])
X_scaled = scaler.fit_transform(X_reshaped)
X_normalized = X_scaled.reshape(X.shape)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_normalized, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42
)

# Data augmentation for minority class
minority_class = np.argmin(np.bincount(y_train))
minority_mask = y_train == minority_class
X_train = np.concatenate([X_train, X_train[minority_mask]], axis=0)
y_train = np.concatenate([y_train, y_train[minority_mask]])

# =============================================
# 6. Feature Selection
# =============================================
print("Calculating feature importance...")
# Flatten time-series for RF importance calculation
X_train_flat = X_train.reshape(X_train.shape[0], -1)

# Train RF to get feature importance
rf = RandomForestClassifier(n_estimators=100, class_weight=class_weight_dict)
rf.fit(X_train_flat, y_train)

# Get importance per original feature (averaged across time steps)
feature_names = ['pos_x','pos_y','pos_z','vel_x','vel_y','vel_z','BC','C_SRP']
n_features = len(feature_names)
n_timesteps = X_train.shape[1]

# Reshape importances to [n_features, n_timesteps] and average across time
importances = rf.feature_importances_.reshape(n_features, n_timesteps).mean(axis=1)

# Plot importance
plt.barh(feature_names, importances)
plt.title("Feature Importance (Averaged Across Time Steps)")
plt.show()

# Select top N features
top_n = 6
selected_indices = np.argsort(importances)[-top_n:]
selected_features = [feature_names[i] for i in selected_indices]
print(f"\nSelected Features: {selected_features}")

# Filter datasets
X_train_selected = X_train[:, selected_indices, :]
X_test_selected = X_test[:, selected_indices, :]

# =============================================
# 7. Voting Classifier with Selected Features
# =============================================
print("\nTraining voting classifier with selected features...")
input_shape_selected = (X_train_selected.shape[1], X_train_selected.shape[2])
model = create_voting_classifier(input_shape_selected, len(classes), class_weight_dict)
model.fit(X_train_selected, y_train)

# Evaluate
y_pred = model.predict(X_test_selected)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le.classes_))

print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f"Confusion Matrix (Top {top_n} Features)")
plt.show()

# Compare accuracy
if 'X_test_full' in locals():
    y_pred_full = model.predict(X_test_full)
    print(f"\nAccuracy Comparison:")
    print(f"All Features: {accuracy_score(y_test, y_pred_full):.4f}")
    print(f"Selected Features: {accuracy_score(y_test, y_pred):.4f}")

"""---"""

import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking
from tensorflow.keras.callbacks import EarlyStopping
import re

# ==========================
# 1. Custom DateTime Parser
# ==========================
def parse_satellite_datetime(dt_str):
    """Parse dates in format: '2022 244 (01 SEP) 01:38:58.810'"""
    try:
        # Extract components using regex
        match = re.match(r"(\d{4})\s+(\d{3})\s+\((\d{2})\s+([A-Za-z]{3})\)\s+(\d{2}:\d{2}:\d{2}\.\d{3})", dt_str)
        if not match:
            return pd.NaT

        year, doy, day, month, time = match.groups()
        month_num = datetime.strptime(month, "%b").month
        base_date = datetime(int(year), 1, 1)
        parsed_date = base_date + pd.Timedelta(days=int(doy)-1)
        time_obj = datetime.strptime(time, "%H:%M:%S.%f").time()
        return datetime.combine(parsed_date.date(), time_obj)
    except Exception as e:
        print(f"Error parsing {dt_str}: {e}")
        return pd.NaT

# =============================
# 2. Data Loading and Cleaning
# =============================
def load_and_prepare_data(csv_path):
    # Load with custom datetime parsing
    df = pd.read_csv(csv_path, converters={
        'MESSAGE TIME (UTC)': parse_satellite_datetime,
        'EPOCH TIME (UTC)': parse_satellite_datetime
    })

    # Clean data
    df = df.dropna(subset=['EPOCH TIME (UTC)'])
    df = df.sort_values(['SATELLITE NUMBER', 'EPOCH TIME (UTC)'])

    # Calculate time differences
    df['time_seconds'] = (df['EPOCH TIME (UTC)'] - df['EPOCH TIME (UTC)'].min()).dt.total_seconds()

    return df

# ===================================
# 3. Time-Series Sequence Generation
# ===================================
def create_satellite_sequences(df, seq_length=12, min_gap_hours=6):
    """
    Creates sequences ensuring:
    - All points from same satellite
    - No large time gaps within sequences
    """
    features = [
        'J2K POS X (KM)', 'J2K POS Y (KM)', 'J2K POS Z (KM)',
        'J2K VEL X (KM/S)', 'J2K VEL Y (KM/S)', 'J2K VEL Z (KM/S)',
        'BALLISTIC COEF (M2/KG)'
    ]

    sequences = []
    labels = []
    metadata = []

    for sat_id, group in df.groupby('SATELLITE NUMBER'):
        group = group.sort_values('time_seconds')
        time_diffs = group['time_seconds'].diff().fillna(0)

        # Split into continuous segments (no large gaps)
        gap_indices = np.where(time_diffs > min_gap_hours * 3600)[0]
        segments = np.split(group, gap_indices)

        for seg in segments:
            if len(seg) < seq_length:
                continue

            # Create sliding windows
            for i in range(0, len(seg) - seq_length + 1):
                seq = seg.iloc[i:i+seq_length]
                sequences.append(seq[features].values)
                labels.append(seq['OBJECT_TYPE'].iloc[0])
                metadata.append({
                    'sat_id': sat_id,
                    'start_time': seq['EPOCH TIME (UTC)'].iloc[0],
                    'end_time': seq['EPOCH TIME (UTC)'].iloc[-1]
                })

    return np.array(sequences), np.array(labels), pd.DataFrame(metadata)

# =============================================
# 4. Main Processing Pipeline
# =============================================
# Load data
df = load_and_prepare_data('gs://betty_101/processed_space_objects_data.csv')

# Generate sequences (12 timesteps, max 6hr gap within sequence)
X, y, meta = create_satellite_sequences(df, seq_length=12, min_gap_hours=6)

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Normalize features per satellite
scaler = StandardScaler()
X_normalized = np.zeros_like(X)
for i in range(X.shape[0]):
    X_normalized[i] = scaler.fit_transform(X[i])

# Train-test split by satellite (not by individual sequences)
sat_ids = meta['sat_id'].unique()
train_sats, test_sats = train_test_split(sat_ids, test_size=0.2, random_state=42)

train_mask = meta['sat_id'].isin(train_sats)
test_mask = meta['sat_id'].isin(test_sats)

X_train, X_test = X_normalized[train_mask], X_normalized[test_mask]
y_train, y_test = y_encoded[train_mask], y_encoded[test_mask]

# =============================================
# 5. LSTM Model Architecture
# =============================================
model = Sequential([
    Masking(mask_value=0, input_shape=(X_train.shape[1], X_train.shape[2])),
    LSTM(128, return_sequences=True, dropout=0.3),
    LSTM(64, dropout=0.3),
    Dense(len(le.classes_), activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train with early stopping
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test, y_test),
    callbacks=[EarlyStopping(patience=10, restore_best_weights=True)]
)

import matplotlib.pyplot as plt

# =============================================
# 6. Evaluation
# =============================================
from sklearn.metrics import classification_report

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

print("Classification Report:")
print(classification_report(y_test, y_pred_classes, target_names=le.classes_))

# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Training History')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()