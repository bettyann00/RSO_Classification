# -*- coding: utf-8 -*-
"""betty.james (25 Apr 2025, 16:25:18)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/data-science-911/locations/us-central1/repositories/86a13778-9dba-452a-b777-f92a8117587c

**LSTM**

Loading data and exploration
"""

import pandas as pd

df = pd.read_csv("gs://betty_101/processed_space_objects_data.csv")

df.head()

print(f"Total records: {len(df)}")

df.info()

missing_values = df.isnull().sum()
print(missing_values)

columns_to_drop = ['LUNAR/SOLAR', 'AVERAGE AP', 'UT1 RATE (MS/DAY)', 'NUTAT', 'MESSAGE_HOUR', 'MESSAGE_DAYOFWEEK', 'EPOCH_HOUR', 'EPOCH_DAYOFWEEK']
df_cleaned = df.drop(columns=columns_to_drop)

df_cleaned.shape

unique_sats = df_cleaned['SATELLITE NUMBER'].nunique()
print(f"\nUnique satellite numbers: {unique_sats}")

if 'OBJECT_TYPE' in df_cleaned.columns:
        obj_type_counts = df_cleaned['OBJECT_TYPE'].value_counts()
        print("\nObject Type Distribution:")
        print(obj_type_counts)

messages_per_sat = df_cleaned['SATELLITE NUMBER'].value_counts()
print("\nMessages per Satellite:")
print(messages_per_sat)

sat_class_dist = df_cleaned.groupby('SATELLITE NUMBER')['OBJECT_TYPE'].agg(lambda x: x.mode()[0]).value_counts()

print(sat_class_dist)

categorical_cols = [col for col in df_cleaned.columns
                       if df_cleaned[col].dtype == 'object'
                       and col != 'OBJECT_TYPE']

print(f"Categorical columns to encode: {categorical_cols}")

def parse_message_time(msg_time_str):
    """Parse MESSAGE TIME (UTC) format: '2022 244 (01 SEP) 03:13:33.000'"""
    try:
        if pd.isna(msg_time_str):
            return pd.NaT

        # Example: '2022 244 (01 SEP) 03:13:33.000' splits into 5 parts:
        # ['2022', '244', '(01', 'SEP)', '03:13:33.000']
        parts = msg_time_str.split()
        if len(parts) != 5:
            raise ValueError(f"Expected 5 parts, got {len(parts)}")

        year = int(parts[0])
        day_of_year = int(parts[1])  # Not used but preserved in parsing

        # Process date parts '(01' and 'SEP)'
        day = int(parts[2].lstrip('('))  # Extract '01' from '(01'
        month_str = parts[3].rstrip(')')  # Extract 'SEP' from 'SEP)'

        month_map = {
            'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,
            'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12
        }

        month = month_map[month_str.upper()]

        # Process time part '03:13:33.000'
        hh, mm, ss = parts[4].split(':')
        hour, minute = int(hh), int(mm)
        seconds = float(ss)
        second = int(seconds)
        microsecond = int((seconds - second) * 1e6)

        return datetime(year, month, day, hour, minute, second, microsecond)

    except Exception as e:
        print(f"Failed to parse '{msg_time_str}': {str(e)}")
        return pd.NaT

print("Sample raw timestamps:")
print(df_cleaned['MESSAGE TIME (UTC)'].head(10).to_list())

test_cases = [
    '2022 244 (01 SEP) 03:13:33.000',
    '2022 244 (01 SEP) 05:43:28.000',
    '2022 244 (01 SEP) 07:17:21.000'
]

from datetime import datetime

for test in test_cases:
    result = parse_message_time(test)
    print(f"{test} â†’ {result} (type: {type(result)})")

from tqdm import tqdm
tqdm.pandas()

df_cleaned['MESSAGE_DATETIME'] = df_cleaned['MESSAGE TIME (UTC)'].progress_apply(parse_message_time)

df_cleaned.drop('MESSAGE TIME (UTC)', axis=1, inplace=True)

if 'EPOCH TIME (UTC)' in df_cleaned.columns:
    # Split into components
    split = df_cleaned['EPOCH TIME (UTC)'].str.split()
    year = split.str[0].astype(int)
    doy = split.str[1].astype(int)
    time = split.str[-1]

    # Parse time
    hh = time.str.split(':').str[0].astype(int)
    mm = time.str.split(':').str[1].astype(int)
    ss = time.str.split(':').str[2].astype(float)

    # Convert to datetime
    df_cleaned['EPOCH_DATETIME'] = (
        pd.to_datetime(year, format='%Y')
        + pd.to_timedelta(doy-1, unit='D')
        + pd.to_timedelta(hh, unit='h')
        + pd.to_timedelta(mm, unit='m')
        + pd.to_timedelta(ss, unit='s')
    )

df_cleaned.drop('EPOCH TIME (UTC)', axis=1, inplace=True)

import numpy as np

def datetime_to_numeric(df_cleaned):
    """Convert datetime columns to multiple numerical features"""

    for col in ['MESSAGE_DATETIME', 'EPOCH_DATETIME']:
        if col in df_cleaned.columns:
            # Extract components
            df_cleaned[f'{col}_EPOCH'] = (df_cleaned[col] - pd.Timestamp("1970-01-01")).dt.total_seconds().astype('float32')
            df_cleaned[f'{col}_DAYOFYEAR'] = df_cleaned[col].dt.dayofyear.astype('int16')
            df_cleaned[f'{col}_HOUR'] = df_cleaned[col].dt.hour.astype('int8')
            df_cleaned[f'{col}_MINUTE'] = df_cleaned[col].dt.minute.astype('int8')

            # Cyclical encoding (for ML models)
            df_cleaned[f'{col}_SIN_DAY'] = np.sin(2 * np.pi * df_cleaned[col].dt.dayofyear / 365).astype('float32')
            df_cleaned[f'{col}_COS_DAY'] = np.cos(2 * np.pi * df_cleaned[col].dt.dayofyear / 365).astype('float32')

            # Drop original datetime column
            df_cleaned = df_cleaned.drop(col, axis=1)

    return df_cleaned

# Apply conversion
df_cleaned = datetime_to_numeric(df_cleaned)

# Check new features
print("New numerical time features:")
print(df_cleaned.filter(like='_EPOCH').head())

# Check dtypes
print("\nData types after conversion:")
print(df.dtypes.value_counts())

df_cleaned.info()

df_cleaned.head()

cat_cols = ['GEOPOTENTIAL', 'DRAG', 'SOLAR RAD PRESS',
           'SOLID EARTH TIDES', 'IN-TRACK THRUST']

for col in cat_cols:
    if col in df.columns:
        unique_vals = df[col].unique()
        print(f"\nColumn: {col}")
        print(f"Data type: {df[col].dtype}")
        print(f"Number of unique values: {len(unique_vals)}")
        print("Sample values:", unique_vals[:10])
        print(f"Missing values: {df[col].isna().sum()}")
    else:
        print(f"\nColumn {col} not found in DataFrame")

for col in cat_cols:
    if col in df.columns and len(df[col].unique()) <= 20:
        print(f"\nValue counts for {col}:")
        print(df[col].value_counts(dropna=False))

df_cleaned = df_cleaned.drop(columns=cat_cols)
print(f"Dropped constant columns: {cat_cols}")

df_cleaned.shape

df_cleaned.info()

X = df_cleaned.drop('OBJECT_TYPE', axis=1).values
y = df_cleaned['OBJECT_TYPE'].values

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import LabelEncoder, StandardScaler
from collections import Counter
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
import gc

# Clear memory
torch.cuda.empty_cache()
gc.collect()

# 1. Robust Dataset Class
class SatelliteDataset(Dataset):
    def __init__(self, features, targets):
        # Convert to numpy first to avoid memory fragmentation
        self.features = np.asarray(features, dtype=np.float32)
        self.targets = np.asarray(targets, dtype=np.int64)

        # Validate shapes
        assert len(self.features) == len(self.targets), "Features and targets length mismatch"

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        # Explicit index checking
        if idx >= len(self):
            raise IndexError(f"Index {idx} out of range for dataset with size {len(self)}")
        return (
            torch.from_numpy(self.features[idx]),
            torch.from_numpy(np.array(self.targets[idx]))
        )

# 2. Data Preparation
def prepare_data(df_cleaned):
    # Check for null values
    assert not df_cleaned.isnull().values.any(), "Data contains null values"

    le = LabelEncoder()
    targets = le.fit_transform(df_cleaned['OBJECT_TYPE'])

    # Temporal features (modify these column names as needed)
    temporal_cols = [col for col in df_cleaned.columns if 'DATETIME' in col or 'EPOCH' in col]

    # Ensure we have temporal features
    assert len(temporal_cols) > 0, "No temporal columns found"

    features = df_cleaned.drop(['OBJECT_TYPE', 'SATELLITE NUMBER'], axis=1)
    scaler = StandardScaler()
    features = scaler.fit_transform(features)

    return features, targets, le, scaler

# 3. Model Architecture (simplified)
class SatelliteModel(nn.Module):
    def __init__(self, input_size, num_classes):
        super().__init__()
        self.temporal_layer = nn.LSTM(input_size, 64, batch_first=True)
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        x = x.unsqueeze(1)  # Add sequence dimension
        lstm_out, _ = self.temporal_layer(x)
        return self.classifier(lstm_out[:, -1, :])

# 4. Training Function
def train_model(df_cleaned):
    # Prepare data
    features, targets, le, scaler = prepare_data(df_cleaned)

    # Create dataset
    dataset = SatelliteDataset(features, targets)

    # Split dataset properly
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    # Create dataloaders with persistent workers
    train_loader = DataLoader(
        train_dataset,
        batch_size=256,
        shuffle=True,
        pin_memory=True,
        num_workers=2,
        persistent_workers=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=512,
        pin_memory=True,
        num_workers=2,
        persistent_workers=True
    )

    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = SatelliteModel(
        input_size=features.shape[1],
        num_classes=len(le.classes_)
    ).to(device)

    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    scaler = GradScaler()

    # Training loop
    best_acc = 0
    for epoch in tqdm(range(20), desc='Epochs'):
        model.train()
        train_loss = 0

        for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc='Training', leave=False)):
            inputs, labels = inputs.to(device), labels.to(device).squeeze()

            optimizer.zero_grad()

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()

        # Validation
        model.eval()
        correct = total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device).squeeze()
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        val_acc = 100 * correct / total
        print(f'Epoch {epoch+1}: Val Acc: {val_acc:.2f}%')

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'New best model saved! (Acc: {val_acc:.2f}%)')

# Run training
if __name__ == "__main__":
    # Assuming df_cleaned is already loaded
    train_model(df_cleaned)

import matplotlib.pyplot as plt

val_accuracies = [83.10, 83.46, 83.96, 84.07, 84.12, 84.42, 84.29, 84.48, 84.18, 84.43, 84.51, 84.45, 84.52, 84.77, 84.83, 84.85, 84.58, 84.63, 84.82, 84.71]

plt.figure(figsize=(10, 5))
plt.plot(val_accuracies, 'o-', label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Training Progress')
plt.axvline(15, color='r', linestyle='--', alpha=0.3, label='Peak at Epoch 16')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

val_accuracies = [83.10, 83.46, 83.96, 84.07, 84.12, 84.42, 84.29, 84.48, 84.18, 84.43, 84.51, 84.45, 84.52, 84.77, 84.83, 84.85, 84.58, 84.63, 84.82, 84.71]

plt.figure(figsize=(10, 5))
plt.plot(val_accuracies, 'o-', label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Training Progress')
plt.axvline(15, color='r', linestyle='--', alpha=0.3, label='Peak at Epoch 16')  # Position is 15 for the 16th epoch
plt.legend()
plt.grid(True)

# Optional: Adjust x-axis ticks to show epoch numbers starting from 1
plt.xticks(range(len(val_accuracies)), range(1, len(val_accuracies)+1))

plt.show()

"""---

**TRANSFORMER**
"""

!pip install optim

import torch.optim as optim  # Import PyTorch's optim module

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt

# 1. Enhanced Dataset Class
class SatelliteDataset(Dataset):
    def __init__(self, features, targets, sequence_length=5):
        self.features = torch.FloatTensor(features)
        self.targets = torch.LongTensor(targets)
        self.seq_len = sequence_length

        # Create sequences
        self.sequences = []
        for i in range(len(self.features) - self.seq_len + 1):
            self.sequences.append((
                self.features[i:i+self.seq_len],
                self.targets[i+self.seq_len-1]  # Predict last element
            ))

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx]

# 2. GRU Model Architecture
class GRUClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, num_layers=2):
        super().__init__()
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3 if num_layers > 1 else 0
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size//2, num_classes)
        )

    def forward(self, x):
        # x shape: [batch, seq_len, features]
        gru_out, _ = self.gru(x)
        # Take last time step's output
        last_out = gru_out[:, -1, :]
        return self.fc(last_out)

# 3. Data Preparation
def prepare_data(df_cleaned, sequence_length=5):
    # Encode targets
    le = LabelEncoder()
    targets = le.fit_transform(df_cleaned['OBJECT_TYPE'])

    # Prepare features
    features = df_cleaned.drop(['OBJECT_TYPE', 'SATELLITE NUMBER'], axis=1)
    scaler = StandardScaler()
    features = scaler.fit_transform(features)

    return features, targets, le, scaler

# 4. Training Function
def train_gru_model(df_cleaned, num_epochs=30):
    # Prepare data with sequences
    features, targets, le, scaler = prepare_data(df_cleaned)
    dataset = SatelliteDataset(features, targets)

    # Split dataset
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    # Create dataloaders
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=128)

    # Initialize model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = GRUClassifier(
        input_size=features.shape[1],
        hidden_size=128,
        num_classes=len(le.classes_)
    ).to(device)

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3)

    # Training loop
    history = {'train_loss': [], 'val_acc': []}

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for sequences, labels in train_loader:
            sequences, labels = sequences.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        # Validation
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for sequences, labels in val_loader:
                sequences, labels = sequences.to(device), labels.to(device)
                outputs = model(sequences)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_acc = 100 * correct / total
        scheduler.step(val_acc)

        # Save history
        history['train_loss'].append(epoch_loss/len(train_loader))
        history['val_acc'].append(val_acc)

        print(f'Epoch {epoch+1}/{num_epochs}: '
              f'Train Loss: {history["train_loss"][-1]:.4f} | '
              f'Val Acc: {val_acc:.2f}% | '
              f'LR: {optimizer.param_groups[0]["lr"]:.2e}')

    # Plot results
    plt.figure(figsize=(12,5))
    plt.subplot(1,2,1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.xlabel('Epoch')
    plt.legend()

    plt.subplot(1,2,2)
    plt.plot(history['val_acc'], label='Val Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()

    plt.tight_layout()
    plt.show()

    return model, history

# Usage
if __name__ == "__main__":
    # df = pd.read_csv('your_data.csv')
    model, history = train_gru_model(df_cleaned)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.cuda.amp import GradScaler, autocast
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from collections import Counter
from tqdm.notebook import tqdm
import pandas as pd
import numpy as np
import gc

# Clear memory
torch.cuda.empty_cache()
gc.collect()

# 1. Enhanced Dataset Class with Oversampling
class SatelliteDataset(Dataset):
    def __init__(self, features, targets, oversample=False):
        self.features = np.asarray(features, dtype=np.float32)
        self.targets = np.asarray(targets, dtype=np.int64)

        if oversample:
            self._oversample_minority_classes()

        assert len(self.features) == len(self.targets), "Mismatched lengths!"

    def _oversample_minority_classes(self):
        class_counts = np.bincount(self.targets)
        max_count = max(class_counts)
        resampled_features = []
        resampled_targets = []

        for class_idx in range(len(class_counts)):
            indices = np.where(self.targets == class_idx)[0]
            num_samples = len(indices)
            if num_samples < max_count:
                repeat_times = max_count // num_samples
                remainder = max_count % num_samples
                resampled_indices = np.repeat(indices, repeat_times)
                resampled_indices = np.concatenate([resampled_indices, indices[:remainder]])
                resampled_features.append(self.features[resampled_indices])
                resampled_targets.append(self.targets[resampled_indices])
            else:
                resampled_features.append(self.features[indices])
                resampled_targets.append(self.targets[indices])

        self.features = np.vstack(resampled_features)
        self.targets = np.concatenate(resampled_targets)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return (
            torch.from_numpy(self.features[idx]),
            torch.from_numpy(np.array(self.targets[idx]))
        )

# Lion Optimizer Implementation (for PyTorch < 2.0)
class Lion(optim.Optimizer):
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        defaults = {'lr': lr, 'betas': betas, 'weight_decay': weight_decay}
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]
                if len(state) == 0:
                    state['exp_avg'] = torch.zeros_like(p)

                exp_avg = state['exp_avg']
                beta1, beta2 = group['betas']

                update = exp_avg * beta1 + grad * (1 - beta1)
                p.add_(-group['lr'] * torch.sign(update))
                exp_avg.mul_(beta2).add_(grad * (1 - beta2))

                if group['weight_decay'] > 0:
                    p.add_(-group['weight_decay'] * group['lr'] * p)

        return loss

# 2. Hybrid Transformer-LSTM Model
class AdvancedSatelliteModel(nn.Module):
    def __init__(self, input_size, num_classes):
        super().__init__()
        # Temporal Feature Extractor
        self.lstm = nn.LSTM(input_size, 64, batch_first=True, bidirectional=True)
        self.transformer_encoder = nn.TransformerEncoderLayer(
            d_model=128, nhead=4, dim_feedforward=256
        )

        # Classifier Head
        self.classifier = nn.Sequential(
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.SiLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        x = x.unsqueeze(1)  # [batch, 1, features]
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, 128]
        transformer_out = self.transformer_encoder(lstm_out)
        return self.classifier(transformer_out[:, -1, :])

# 3. Data Preparation
def prepare_data(df_cleaned):
    assert not df_cleaned.isnull().values.any(), "Data contains null values"

    le = LabelEncoder()
    targets = le.fit_transform(df_cleaned['OBJECT_TYPE'])

    features = df_cleaned.drop(['OBJECT_TYPE', 'SATELLITE NUMBER'], axis=1)
    scaler = StandardScaler()
    features = scaler.fit_transform(features)

    return features, targets, le, scaler

# 4. Enhanced Training Pipeline
def train_advanced_model(df_cleaned):
    # Data Preparation
    features, targets, le, scaler = prepare_data(df_cleaned)

    # Stratified Train/Val Split
    train_indices, val_indices = train_test_split(
        np.arange(len(targets)),
        test_size=0.2,
        random_state=42,
        stratify=targets
    )

    # Class Weighting (Training Data Only)
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(targets[train_indices]),
        y=targets[train_indices]
    )
    class_weights = torch.tensor(class_weights, dtype=torch.float32)

    # Create Datasets
    train_dataset = SatelliteDataset(
        features[train_indices],
        targets[train_indices],
        oversample=True
    )
    val_dataset = SatelliteDataset(
        features[val_indices],
        targets[val_indices]
    )

    # Dataloaders
    train_loader = DataLoader(
        train_dataset, batch_size=256, shuffle=True,
        pin_memory=True, num_workers=4, persistent_workers=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=512,
        pin_memory=True, num_workers=4
    )

    # Model Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AdvancedSatelliteModel(
        input_size=features.shape[1],
        num_classes=len(le.classes_)
    ).to(device)

    # Optimizer Selection (Auto-handle Lion availability)
    try:
        optimizer = optim.Lion(model.parameters(), lr=0.001, weight_decay=0.01)
        print("Using native Lion optimizer (PyTorch 2.0+)")
    except AttributeError:
        optimizer = Lion(model.parameters(), lr=0.001, weight_decay=0.01)
        print("Using custom Lion implementation")

    # Scheduler and Loss
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=5, T_mult=1
    )
    criterion = nn.CrossEntropyLoss(
        weight=class_weights.to(device),
        label_smoothing=0.1
    )
    scaler = GradScaler()

    # Training Loop
    best_acc = 0
    early_stop_counter = 0

    for epoch in tqdm(range(50), desc='Training'):
        model.train()
        train_loss = 0

        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):
            inputs, labels = inputs.to(device), labels.to(device).squeeze()

            optimizer.zero_grad()

            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()

        # Validation
        model.eval()
        val_acc = evaluate_model(model, val_loader, device)

        # Scheduler step
        scheduler.step()

        # Early stopping
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_model_advanced.pth')
            early_stop_counter = 0
        else:
            early_stop_counter += 1
            if early_stop_counter >= 5:
                print(f"Early stopping at epoch {epoch+1}!")
                break

        print(f"Epoch {epoch+1}: Val Acc = {val_acc:.2f}% | Best = {best_acc:.2f}%")

def evaluate_model(model, data_loader, device):
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device).squeeze()
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    return 100 * correct / total

# 5. Test-Time Augmentation (TTA)
def predict_with_tta(model, inputs, device, n_augments=5):
    model.eval()
    all_outputs = []

    for _ in range(n_augments):
        noisy_inputs = inputs + torch.randn_like(inputs) * 0.01
        with torch.no_grad():
            outputs = model(noisy_inputs.to(device))
        all_outputs.append(outputs.softmax(dim=1))

    return torch.stack(all_outputs).mean(dim=0)

if __name__ == "__main__":
    train_advanced_model(df_cleaned)